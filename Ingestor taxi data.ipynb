{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98342a3e-efe7-4e93-8094-9ac8eab3197f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# first run (21/03/2025)\n",
    "- year 2009\n",
    "- one base existing in this year, each month around 500mb with 8 minutes total time\n",
    "- Using 1 Driver 14 GB Memory, 4 Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8d2f9ed-f9e5-4be4-a4c9-1a64f7845a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Done with this one, did some parametrizations but there`s a request limit to download. Also I saw that they have an api on NYC data. Moving to study with dbdemos instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50d84ae-ec91-489d-b047-77a7433cb7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure you are running this in a Databricks notebook\n",
    "secret = dbutils.secrets.get(scope=\"kvlagodedados\", key=\"blobstorage\")\n",
    "display(secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87998771-f363-4798-90d8-c533757b79a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from retry import retry\n",
    "\n",
    "class IngestorTaxiData:\n",
    "    def __init__(self, path_to_json):\n",
    "        self.parameters_dict, self.url_dict = self.get_parameters(path_to_json)\n",
    "        self.blob_service_client = BlobServiceClient(\n",
    "            account_url=f\"https://{self.parameters_dict['storage_account_name']}.blob.core.windows.net\",\n",
    "            credential=self.parameters_dict['storage_account_key']\n",
    "        )\n",
    "\n",
    "    def get_parameters(self, path_to_json):\n",
    "        # reading parameters from json\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(path_to_json)\n",
    "        df = df.selectExpr(\"parameters.*\")\n",
    "        # setting up parameters dict\n",
    "        years = list(range(2009, datetime.now().year + 1))\n",
    "        months = [f\"{m:02d}\" for m in range(1, 13)]  # 01 to 12\n",
    "        parameters_dict = df.first().asDict()\n",
    "        parameters_dict['years'] = years\n",
    "        parameters_dict['months'] = months\n",
    "        # dict to iterate throught all urls that are needed to download\n",
    "        url_dict = {\n",
    "            f\"staging/{trip}/{year}/{month}/{trip}_tripdata_{year}-{month}.parquet\":\n",
    "            f\"{parameters_dict['base_url']}{trip}_tripdata_{year}-{month}.parquet\"\n",
    "            for year in parameters_dict['years'] for month in parameters_dict['months'] for trip in parameters_dict['trip_types']\n",
    "        }\n",
    "        # base url\n",
    "        return parameters_dict, url_dict\n",
    "\n",
    "    def error_handler(self, response, path_file, file_url, file_data):\n",
    "        # Check HTTP response status and validate file content\n",
    "        error_message = f\"Failed to get data from {file_url}: HTTP {response.status_code}\" if response.status_code != 200 else f\"Failed to get data from: {file_data.decode('utf-8')}\"\n",
    "        log_path_file = f\"staging/logs/upload_errors_staging_{path_file.replace('/', '_')}_{datetime.now().strftime('%Y%m%d%H%M%S')}.log\"\n",
    "        log_blob_client = self.blob_service_client.get_blob_client(container=self.parameters_dict['container_name'], blob=log_path_file)\n",
    "        log_content = f\"Error: {str(error_message)}\\nURL: {file_url}\\n\"\n",
    "        log_blob_client.upload_blob(log_content, overwrite=True)\n",
    "        raise Exception(error_message)\n",
    "\n",
    "    def upload_file(self, blob_client, file_data):\n",
    "        blob_client.upload_blob(file_data, overwrite=True)\n",
    "\n",
    "    @retry(tries=3, delay=2)\n",
    "    def get_data(self, path_file, file_url):\n",
    "        try:\n",
    "            # Check if the blob already exists\n",
    "            blob_client = self.blob_service_client.get_blob_client(\n",
    "                container=self.parameters_dict['container_name'], blob=path_file\n",
    "            )\n",
    "            if blob_client.exists():\n",
    "                print(f\"Blob {path_file} already exists. Skipping download and upload.\")\n",
    "                return\n",
    "\n",
    "            # Download the file from the URL\n",
    "            response = requests.get(file_url)\n",
    "            file_data = response.content\n",
    "\n",
    "            # Handle errors\n",
    "            if response.status_code != 200 or b\"<Error>\" in file_data:\n",
    "                self.error_handler(response, path_file, file_url, file_data)\n",
    "            \n",
    "            # Upload the file to Azure Blob Storage\n",
    "            self.upload_file(blob_client, file_data)\n",
    "            print(f\"Uploaded {path_file} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def ingestor(self, num_workers):\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = [executor.submit(self.get_data, path_file, file_url) for path_file, file_url in self.url_dict.items()]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "        print(\"File upload process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ba9722-9b79-4e5c-8ca3-26d2d04eea24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usage in parallel\n",
    "path_to_json = \"abfss://lagodedadosv1@lagodedadosalttab.dfs.core.windows.net/parametros/parameters.json\"\n",
    "data_ingestor = IngestorTaxiData(path_to_json)\n",
    "data_ingestor.ingestor(num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c051e0-93ab-4683-ab86-7f391bb9d010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"abfss://lagodedadosv1@lagodedadosalttab.dfs.core.windows.net/staging/*\")\n",
    "display(df.where(\"value like '%https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-08.parquet%'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b6f163-e2a8-4168-88ec-85b6778f4d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Next steps: \n",
    "\n",
    "- Hide secret key using key vault (done)\n",
    "- parametrize everything (done)\n",
    "- improve ingestion speed (parallelize) (done but we have request limits)\n",
    "- add meta data (stop this project)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestor taxi data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
